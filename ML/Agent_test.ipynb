{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BABY agent-like but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_squares(a: int, b: int) -> dict:\n",
    "    result = (a**2) * (b**2)\n",
    "    parity = \"even\" if result % 2 == 0 else \"odd\"\n",
    "    return {\"result\": result, \"parity\": parity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathAgent:\n",
    "    def __init__(self, tool_function):\n",
    "        self.tool = tool_function\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"\n",
    "        A simple parser: it looks for numbers in the message.\n",
    "        Example: 'compute for a=3 b=4'\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        # Extract integers\n",
    "        nums = list(map(int, re.findall(r\"-?\\d+\", message)))\n",
    "\n",
    "        if len(nums) < 2:\n",
    "            return \"Please provide two integers a and b.\"\n",
    "\n",
    "        a, b = nums[0], nums[1]\n",
    "\n",
    "        # Call the tool (your custom function)\n",
    "        output = self.tool(a, b)\n",
    "\n",
    "        result = output[\"result\"]\n",
    "        parity = output[\"parity\"]\n",
    "\n",
    "        # Response\n",
    "        return (\n",
    "            f\"Given a = {a} and b = {b}, I computed:\\n\\n\"\n",
    "            f\"• a² = {a**2}\\n\"\n",
    "            f\"• b² = {b**2}\\n\"\n",
    "            f\"• a²·b² = {result}\\n\\n\"\n",
    "            f\"This number is **{parity}**.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a = 3 and b = 5, I computed:\n",
      "\n",
      "• a² = 9\n",
      "• b² = 25\n",
      "• a²·b² = 225\n",
      "\n",
      "This number is **odd**.\n"
     ]
    }
   ],
   "source": [
    "agent = MathAgent(multiply_squares)\n",
    "print(agent.chat(\"hey, can you compute this for a=3 and b=5?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given a = 3 and b = 5, I computed:\\n\\n• a² = 9\\n• b² = 25\\n• a²·b² = 225\\n\\nThis number is **odd**.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"a=3 and b=5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More serious agent with RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defnine the function and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_two_level(delta, omega, T=1.0):\n",
    "    \"\"\"\n",
    "    Returns the transition probability from |0> to |1>.\n",
    "    \"\"\"\n",
    "    # Pauli matrices\n",
    "    sx = np.array([[0, 1], [1, 0]], dtype=complex)\n",
    "    sz = np.array([[1, 0], [0, -1]], dtype=complex)\n",
    "    \n",
    "    H = (delta/2) * sz + omega * sx\n",
    "\n",
    "    # Time evolution: U = exp(-i H T)\n",
    "    eigvals, eigvecs = np.linalg.eigh(H)\n",
    "    U = eigvecs @ np.diag(np.exp(-1j * eigvals * T)) @ eigvecs.conj().T\n",
    "\n",
    "    psi0 = np.array([1,0], dtype=complex)\n",
    "    psif = U @ psi0\n",
    "\n",
    "    prob = np.abs(psif[1])**2\n",
    "    return float(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv:\n",
    "    def __init__(self):\n",
    "        # Discrete action space\n",
    "        self.deltas = np.linspace(-5, 5, 11)\n",
    "        self.omegas = np.linspace(0, 5, 6)\n",
    "        \n",
    "        # All (Δ, Ω) combinations\n",
    "        self.actions = [(d,o) for d in self.deltas for o in self.omegas]\n",
    "        self.n_actions = len(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return np.array([0.0])  # dummy state\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        delta, omega = self.actions[action_idx]\n",
    "        reward = evolve_two_level(delta, omega)\n",
    "        done = True  # one-step episode\n",
    "        return np.array([0.0]), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = QuantumEnv()\n",
    "n_actions = env.n_actions\n",
    "\n",
    "policy_net = DQN(n_actions)\n",
    "target_net = DQN(n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "gamma = 0.99\n",
    "eps = 1.0\n",
    "eps_decay = 0.995\n",
    "\n",
    "def select_action(state, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.randint(n_actions)\n",
    "    with torch.no_grad():\n",
    "        q = policy_net(torch.tensor(state, dtype=torch.float32))\n",
    "        return q.argmax().item()\n",
    "\n",
    "# Training\n",
    "for episode in range(2000):\n",
    "    state = env.reset()\n",
    "    action = select_action(state, eps)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Compute TD target\n",
    "    q_values = policy_net(torch.tensor(state, dtype=torch.float32))\n",
    "    q_value = q_values[action]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_q = reward  # because done=True after 1 step\n",
    "\n",
    "    loss = (q_value - target_q)**2\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    eps *= eps_decay\n",
    "    eps = max(eps, 0.05)\n",
    "\n",
    "# Sync target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_solve():\n",
    "    state = np.array([0.0])\n",
    "    with torch.no_grad():\n",
    "        qvals = policy_net(torch.tensor(state, dtype=torch.float32))\n",
    "        best_action = qvals.argmax().item()\n",
    "    \n",
    "    delta, omega = env.actions[best_action]\n",
    "    prob = evolve_two_level(delta, omega)\n",
    "\n",
    "    print(f\"Best Δ = {delta:.3f}\")\n",
    "    print(f\"Best Ω = {omega:.3f}\")\n",
    "    print(f\"Transition probability P = {prob:.4f}\")\n",
    "    return delta, omega, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Δ = 0.000\n",
      "Best Ω = 5.000\n",
      "Transition probability P = 0.9195\n"
     ]
    }
   ],
   "source": [
    "delta_opt, omega_opt, prob_opt = agent_solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumChatAgent:\n",
    "    def __init__(self, policy_net, env):\n",
    "        self.policy_net = policy_net\n",
    "        self.env = env\n",
    "        \n",
    "        # Compute optimal action once at initialization\n",
    "        self.state = np.array([0.0])\n",
    "        with torch.no_grad():\n",
    "            qvals = self.policy_net(torch.tensor(self.state, dtype=torch.float32))\n",
    "        self.best_action = qvals.argmax().item()\n",
    "        self.best_delta, self.best_omega = env.actions[self.best_action]\n",
    "        self.best_prob = evolve_two_level(self.best_delta, self.best_omega)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 1. Compute probability \n",
    "    # -------------------------------\n",
    "    def prob(self, delta, omega):\n",
    "        return evolve_two_level(delta, omega)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 2. Compare with optimal\n",
    "    # -------------------------------\n",
    "    def compare(self, delta, omega):\n",
    "        p = self.prob(delta, omega)\n",
    "        if p > self.best_prob:\n",
    "            return f\"Surprisingly, Δ={delta}, Ω={omega} gives higher probability ({p:.4f}) than my learned optimum ({self.best_prob:.4f}).\"\n",
    "        elif np.isclose(p, self.best_prob, atol=1e-3):\n",
    "            return f\"Δ={delta}, Ω={omega} gives probability {p:.4f}, which is basically as good as the optimal I learned ({self.best_prob:.4f}).\"\n",
    "        else:\n",
    "            return f\"Δ={delta}, Ω={omega} gives probability {p:.4f}, which is lower than my optimal value {self.best_prob:.4f}.\"\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 3. General chat interface\n",
    "    # -------------------------------\n",
    "    def chat(self, message):\n",
    "        msg = message.lower()\n",
    "        \n",
    "        # ask for best parameters\n",
    "        if \"best\" in msg:\n",
    "            return (f\"My optimal values are:\\n\"\n",
    "                    f\"Δ = {self.best_delta:.3f}, Ω = {self.best_omega:.3f}\\n\"\n",
    "                    f\"with transition probability {self.best_prob:.4f}.\")\n",
    "        \n",
    "        # ask for probability\n",
    "        if \"prob\" in msg or \"probability\" in msg:\n",
    "            # extract numbers from message\n",
    "            import re\n",
    "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", msg)\n",
    "            if len(nums) >= 2:\n",
    "                delta = float(nums[0])\n",
    "                omega = float(nums[1])\n",
    "                p = self.prob(delta, omega)\n",
    "                return f\"For Δ={delta} and Ω={omega}, the transition probability is {p:.4f}.\"\n",
    "            else:\n",
    "                return \"To compute a probability, please give me Δ and Ω.\"\n",
    "        \n",
    "        # ask for comparison\n",
    "        if \"compare\" in msg or \"better\" in msg:\n",
    "            import re\n",
    "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", msg)\n",
    "            if len(nums) >= 2:\n",
    "                delta = float(nums[0])\n",
    "                omega = float(nums[1])\n",
    "                return self.compare(delta, omega)\n",
    "            else:\n",
    "                return \"Please give Δ and Ω so I can compare them to the optimal values.\"\n",
    "        \n",
    "        return \"I can compute probabilities, compare to optimal, or tell you my best learned control values. Ask me something like: 'What is the probability for Δ=2, Ω=1?' or 'What are your best parameters?'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent = QuantumChatAgent(policy_net, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My optimal values are:\n",
      "Δ = 0.000, Ω = 5.000\n",
      "with transition probability 0.9195.\n"
     ]
    }
   ],
   "source": [
    "print(chat_agent.chat(\"What are your best parameters?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Δ=2.0 and Ω=1.5, the transition probability is 0.6557.\n"
     ]
    }
   ],
   "source": [
    "print(chat_agent.chat(\"Give me the probability for delta=2 and omega=1.5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ=0.0, Ω=5.0 gives probability 0.9195, which is basically as good as the optimal I learned (0.9195).\n"
     ]
    }
   ],
   "source": [
    "print(chat_agent.chat(\"Compare delta=0 omega 5.0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can compute probabilities, compare to optimal, or tell you my best learned control values. Ask me something like: 'What is the probability for Δ=2, Ω=1?' or 'What are your best parameters?'.\n"
     ]
    }
   ],
   "source": [
    "print(chat_agent.chat(\"hello there\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
